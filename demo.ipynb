{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33f4049-ac4b-4df4-9da9-d5f909ecace0",
   "metadata": {},
   "source": [
    "# Project summary:\n",
    "\n",
    "This project was initally inspired by [bol.com's serenade project](https://github.com/bolcom/serenade) which offers a fast implementation of the [V-SKNN algorithm](https://arxiv.org/abs/1803.09587), able to serve online customers with personalised recommendations.  \n",
    "The key figures highlighted in the serenade paper show a P90 latency of 7ms and a throughput of 600 requests per second. Serenade reviews up to 5000 historical sessions (neighbours) per item in the current session (up to 10).    \n",
    "The serenade implmentation runs many computations sequentially, while many of those have no interdependencies, suggesting that better performances could be reached with GPU paralelization.  \n",
    "This is where the [NVIDIA RAPIDS](https://developer.nvidia.com/rapids) suite of libraries come into play.  This project offers a GPU implementation of V-SKNN aiming to reach similar or better performances than Serenade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591fda4-ee73-48cc-a21a-e569be8d97fa",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "### V-sKNN short example:\n",
    "Let's say the user has a session comprising to three ordered items `[item_1, item_2, item_3]`, with `item_3` being the most recent. The algorithm will perfom the following steps:\n",
    "* **Lookup candiates (neighbor) sessions:** The algorithm is going to retrieve the historical sessions that contain the item *i* for each item in the input session. **hyperparameter** number of historical sessions per item, `max_sessions_per_item` set to 5000 in this example  \n",
    "* **Calculate the historical session similarity:** This is a step that we want to paralelise as session similarities do not depend on each other. There are many different methods that could be considered here, but one of the key feature of V-sKNN is that each item in the input session carries a different weight. In this example we use a linear decay with weights of `0.33, 0.66` and `1.0` for items  `item_1`, `item_2` and `item_3` respectively. Each session similarity is calcuated as the sum of weights of items that are also found in the input sessions.  **hyperparameter:** weight functions. We use a linear deay in this example.\n",
    "* **Keep the top-k most similar sessions:** The top K most similar sessions are kept and the rest is ignored.  **hyperparameter:** value of `top_k`, set to 100 in this example.\n",
    "* **Calculate item similarity:** each items appearing in the K most similar session is considered for recommendation. The score of each item is the sum of the similarities of sessions that contain the item.  \n",
    "\n",
    "The same \"similarity\" method is used twice: First to calculate session similarities using item weights, then to calculate item similarities using session weights. The same function is re-used in both cases.  \n",
    "\n",
    "### GPU implementation challenges and solutions\n",
    "\n",
    "* **indexing historical data:** Implementations of V-sKNN such as Serenade assume the presence of an efficient key-value store to retrieve the historical session data.  This was experimented with in previous iterations of this project (and could remain a valid option), but the transfer of data from host to device became one of the algorithm's main bottleneck.  So effort was put into making sure that the historical data can be kept and efficiently read on the device.  The current solution is convert the `item_id` and `session_id` into contiguous integers, so that the data for item (session) *i* can be found at the *i-th* row of an array. In order to also keep the memory usage reasonable, we cannot use a 2-D array. Instead, the index *i* enables to lookup the starting and ending position of the historical data for item (session) *i* in another array which contains the values (no padding necessary). A [CuPy RawKernel](https://docs.cupy.dev/en/stable/reference/generated/cupy.RawKernel.html) was created for this operation: It copies the necessary values from the historical data stored on device, into a temporary \"buffer\". \n",
    "* **similarity function:** The similarity of sessions (items) can be loosely described as a \"group by and sum\" operation. However, a cudf groupby was giving performances that were below this proejct's targets, another CuPy `RawKernel` was created here for a groupby operation on the previously mentioned values \"buffer\". \n",
    "* **unique items:** For the \"groupby\" kernel to work, we need a list of unique items (sessions). [CuPy unique](https://docs.cupy.dev/en/stable/reference/generated/cupy.unique.html?highlight=unique#cupy.unique) function does not yet support the `axis` option. A quick rip-off of the CuPy function was built for the occasion. This project can also be an opportunity to add the support for `axis` in CuPy unique.\n",
    "* **converting the item (session) values to integer idx**. At the moment this is done with a simple python dictionary, but could be improved.\n",
    "\n",
    "### Validation:\n",
    "The algorithm was validated by replicating the results found in https://github.com/rn5l/session-rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fd63efc-7451-4a2c-853e-75d6578fbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import os\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from vs_knn.vs_knn import CupyVsKnnModel\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2b4f2",
   "metadata": {},
   "source": [
    "## Dataset: RSC15 \"*Yoochoose*\" data\n",
    "The public *RSC15* e-commerce dataset is one of the largest publicly available e-commerce datasets, which makes it a good choice for illustrating the present project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fbec6-3321-4eaa-96e2-79995772c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget  # wget missing from the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383238fe-979c-4c85-b655-cc58fe1b0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wget import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbec366-fc6c-4978-a492-b4b5871bc69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 99% [1480630272 / 1486798186] bytes"
     ]
    }
   ],
   "source": [
    "dataset_filepath = 'archive/yoochoose-clicks.dat'\n",
    "\n",
    "def bar_progress(current, total, width=80):\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "\n",
    "if not os.path.isfile(dataset_filepath):\n",
    "    download(\"https://storage.googleapis.com/jcrousse-vsknn-rapids/yoochoose-clicks.dat\", out=dataset_filepath, bar=bar_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a87bf-2220-420f-99d4-8f27f11d7bbb",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The algorithm expects to receive a cudf dataframe with 3 columns: `item_id`, `session_id` and `timestamp`. The order does not matter.  \n",
    "The algorithm also does works with int or string `item_id` and `session_id`. We could aim to improve the performances by putting more restrictions on the input format, but at this stage we prefer a solution that is more flexible.  \n",
    "Different public datasets may require a different loading function. Here we implement one for the original RSC15 dataset to showcase an end-to-end example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac8b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath, columns=None, delimiter=','):\n",
    "    import cudf\n",
    "    columns = ['session_id', 'timestamp', 'item_id'] if columns is None else columns\n",
    "    return cudf.read_csv(filepath,\n",
    "                         usecols=[0, 1, 2],\n",
    "                         dtype={\n",
    "                             'session_id': cp.dtype('int32'),\n",
    "                             'item_id': cp.dtype('int32'),\n",
    "                             'timestamp': cp.dtype('O')\n",
    "                         },\n",
    "                         delimiter=delimiter,\n",
    "                         names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bcf7345-6804-4b35-862a-d5e833d30ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = 'archive/yoochoose-clicks.dat'\n",
    "\n",
    "yoochoose_data = read_dataset(dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38dbc467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset contains 33M rows,  with 9M  sessions and 53K items \n",
      "Original file size: 1487Mb\n"
     ]
    }
   ],
   "source": [
    "n_rows = yoochoose_data.shape[0]\n",
    "n_sessions = len(yoochoose_data['session_id'].unique())\n",
    "n_items = len(yoochoose_data['item_id'].unique())\n",
    "filesize = os.path.getsize(dataset_filepath)\n",
    "\n",
    "print(f\"the dataset contains {round(n_rows / 10 ** 6)}M rows, \", \n",
    "      f\"with {round(n_sessions / 10 ** 6)}M \", \n",
    "      f\"sessions and {round(n_items / 10 ** 3)}K items\",\n",
    "      f\"\\nOriginal file size: {round(filesize / 10 ** 6)}Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d62e8f-d689-40be-8715-4e6a663c4f0d",
   "metadata": {},
   "source": [
    "### Data size:\n",
    "A trained V-SKNN model needs to be able to lookup historical sessions. The data found in the dataset above (1.5GB) will have to be stored on the GPU device. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a54028",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Format\n",
    "Internally, the model keeps various arrays of values per `item_id` and `session_id`. It uses integer values between 1 and *N_items* to represent `item_id` (between 1 and *N_sessions* to represent`session_id`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e548c0b",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "The dataset covers 183 days, we will use the first 180 days as train set, and the remaining 3 days as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c8fbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoochoose_data['day'] = yoochoose_data['timestamp'].str.slice(start=0, stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c7da77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2014-04-01\n",
      "1      2014-04-02\n",
      "2      2014-04-03\n",
      "3      2014-04-04\n",
      "4      2014-04-05\n",
      "          ...    \n",
      "178    2014-09-26\n",
      "179    2014-09-27\n",
      "180    2014-09-28\n",
      "181    2014-09-29\n",
      "182    2014-09-30\n",
      "Name: day, Length: 183, dtype: object\n"
     ]
    }
   ],
   "source": [
    "all_days = yoochoose_data['day'].unique()\n",
    "train_days = all_days[0:180]\n",
    "print(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2c4a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = yoochoose_data[yoochoose_data['day'].isin(train_days)][['session_id', 'timestamp', 'item_id']]\n",
    "test_df = yoochoose_data[~yoochoose_data['day'].isin(train_days)][['session_id', 'timestamp', 'item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69175e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662620e2",
   "metadata": {},
   "source": [
    "## Model train\n",
    "training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eed94b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CupyVsKnnModel(top_k=100, max_sessions_per_items=5000, max_item_per_session=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52447474-8667-443e-a850-0aa01dd8098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del yoochoose_data\n",
    "gc.collect()\n",
    "mempool = cp.get_default_memory_pool()\n",
    "mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5661752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device memory footprint for index objects: 202.53 Mb)\n",
      "trained the model in 5.978278398513794 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model.train(train_df)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"trained the model in {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56e4e6c6-5803-4d1d-838f-656760d87f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "gc.collect()\n",
    "mempool = cp.get_default_memory_pool()\n",
    "mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d940168-503a-461e-b296-c606a3457ffd",
   "metadata": {},
   "source": [
    "## Test set\n",
    "To evaluate the model response time and throughput, we generate an easy to use test set. Each test session is a python list with item_id.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "afc37e7d-3105-4bac-9c20-1e095816457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_examples(test_set):\n",
    "    test_array = test_set \\\n",
    "        .drop('timestamp', axis=1) \\\n",
    "        .groupby('session_id') \\\n",
    "        .agg({'item_id': 'collect'})['item_id']\\\n",
    "        .to_pandas()\\\n",
    "        .values\n",
    "    return test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a3d97",
   "metadata": {},
   "source": [
    "## Testing the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b7bf5-2ce0-4552-8137-da3688b83ae4",
   "metadata": {},
   "source": [
    "Function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63f2828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sessions_array = get_test_examples(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d1971d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_to_xy(items_in_session):\n",
    "    return (items_in_session[-10:-1], items_in_session[-1]) if len(items_in_session) > 1 else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aebe8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_a_model(model, test_data, batch_size=1):\n",
    "    \n",
    "    total_hits = 0\n",
    "    n_treated = 0\n",
    "    hr20 = 0\n",
    "    \n",
    "    pbar = tqdm(test_data)\n",
    "    \n",
    "    predict_time = []\n",
    "\n",
    "    for test_session in pbar:\n",
    "        x, y = session_to_xy(test_session)\n",
    "        if x is not None:\n",
    "            x = x[-10:]\n",
    "            prediction = model.predict([x] * batch_size)\n",
    "            items_pred, item_scores = prediction['predicted_items'][0], prediction['scores'][0]\n",
    "            predict_time.append(prediction['total_time'])\n",
    "            n_treated += 1\n",
    "            if len(items_pred) > 0:\n",
    "                selection = cp.flip(cp.argsort(item_scores)[-20:])\n",
    "                items_rec = items_pred[selection]\n",
    "\n",
    "                if y in items_rec:\n",
    "                    total_hits += 1\n",
    "                    hr20 = total_hits / n_treated\n",
    "                    pbar.set_postfix({'HR@20': hr20})\n",
    "\n",
    "    time_per_iter = pbar.format_dict['elapsed'] / pbar.format_dict['n']\n",
    "    avg_latency = round(sum(predict_time) / len(predict_time) * 1000, 2)\n",
    "    p90_latency = round(np.quantile(np.array(predict_time), 0.9) * 1000, 2)\n",
    "    throughput = round((n_treated * batch_size) / sum(predict_time))\n",
    "    print(f\"total inference time: {sum(predict_time)}, average latency: {avg_latency}ms average  Q90: {p90_latency}ms. throughput {throughput} items per second.\")\n",
    "\n",
    "    return time_per_iter, hr20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5b8c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 181.04it/s, HR@20=0.663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time: 2.2565886974334717, average latency: 6.96ms average  Q90: 10.98ms. throughput 144 items per second.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "itertime_rd, hr_rd = test_a_model(model, test_sessions_array[:500], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7626-f91f-4ab7-8d64-5b59778e7bb8",
   "metadata": {},
   "source": [
    "## Performance summary:\n",
    "\n",
    "* Q90 latency: ~10ms\n",
    "* Throughput: ~150 / sec\n",
    "* Training time: ~6s\n",
    "\n",
    "[smaller datasets such as this one](https://github.com/rn5l/session-rec/blob/master/data/rsc15/prepared/yoochoose-clicks-100k_train_full.txt) can obviously yield much better performances:  \n",
    "With minibatches of size 20: `total inference time: 13.142018795013428, average latency: 13.16ms average  Q90: 16.88msthroughput 1520 items per second.`  \n",
    "With 1 item at a time: `total inference time: 2.819638252258301, average latency: 2.82ms average  Q90: 3.17msthroughput 354 items per second.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68808afd-90d3-43b4-ac61-6695a2b31c33",
   "metadata": {},
   "source": [
    "## Future work:\n",
    "* **Optimizating further:** \n",
    "    * **Padding**: I currently use 0 padding for the value buffers, which can lead to queries taking up to 1GB of GPU memory, as well as a large number of threads. For many queries, the number of 0 values can be as high as 90%. By using a different indexing approach (more similar to the approach used for retrieving values).\n",
    "    * **faster name to idx conversion**: The name to idx conversion becomes the bottleneck as the batch size increases.  Other solutions could be investigated such as [sklearn's fast dict](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_fast_dict.pyx)\n",
    "* **More paramatrization options on V-sKnn:** More weighting functions, tie-break approaches, distance functions, ... \n",
    "* **Extend algorithm:** STAN, V-STAN, .... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bfe58-6d7a-469d-86fc-a7456aaf23d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
